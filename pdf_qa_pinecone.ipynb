{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install langchain pinecone-client \"unstructured[local-inference]\"\t# install if needed\n",
    "#!pip3 install \"detectron2@git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 41.35it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 25.70it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1307.25it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 35.36it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 31.75it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 206.17it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 29.27it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 68.39it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 75.91it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 109.27it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 311.75it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 184.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s) with 18443 characters, and split into 26 split-documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader\n",
    "\n",
    "# default load strategy is \"hi_res\", which has better performance but requires detectron2 and can be slow. To be fast and simple, use strategy=\"fast\", \n",
    "loader = UnstructuredPDFLoader(\"./em_lab.pdf\")\t\n",
    "data = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(data)\n",
    "print(f'Loaded {len(data)} document(s) with {len(data[0].page_content)} characters, and split into {len(texts)} split-documents.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import pinecone\n",
    "\n",
    "OPENAI_API_KEY = 'YOUR OPENAI API KEY'\n",
    "PINECONE_API_KEY = 'YOUR PINECONE API KEY'  # look for it at app.pinecone.io\n",
    "PINECONE_API_ENV = 'us-east4-gcp'           # next to pinecone api key in console\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,  \n",
    "    environment=PINECONE_API_ENV  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"langchain2\" # the index name which can be stored in pinecone.io\n",
    "docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo', openai_api_key=OPENAI_API_KEY)\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is magnetic field and has the document discussed anything about it?\"\n",
    "# similarity search narrows down to most similar 'texts' and can speed up the response\n",
    "docs = docsearch.similarity_search(query, include_metadata=True)\t\n",
    "# the answer will be provided after the following line\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the key idea of the context?\"\n",
    "# if you know the most relvant parts of the 'texts', you can input the indexes here. Or if the file is not too large, you can search in 'texts' without any index\n",
    "chain.run(input_documents=texts[0:5], question=query)\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
