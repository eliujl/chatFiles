{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iPython notebook for conversational retrieval/Q&A with multiple Word documents using Langchain, OpenAI (API key required), and Pinecone (API key required). It adds the vectors obtained from the documents to a Pinecone index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install langchain pinecone-client\t# install if needed\n",
    "#!pip3 install \"detectron2@git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load multiple docx file with given filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.document_loaders import UnstructuredWordDocumentLoader\n",
    "\n",
    "file_paths = [\"./R1-51-short.docx\",\"./Rdoc.docx\"]\n",
    "all_texts = []\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "for file in file_paths:\n",
    "    loader = UnstructuredWordDocumentLoader(file)\n",
    "    data = loader.load()\n",
    "    texts = text_splitter.split_documents(data)\n",
    "    print(\n",
    "        f'Loaded {len(data)} document(s) with {len(data[0].page_content)} characters, and split into {len(texts)} split-documents.')\n",
    "    all_texts.extend(texts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Open AI API key (from .bashrc, Windows environment variables, etc. Or .env) and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set up Pinecone env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import pinecone\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "# initialize pinecone\n",
    "PINECONE_API_KEY = os.environ['PINECONE_API_KEY']\n",
    "PINECONE_API_ENV = os.environ['PINECONE_API_ENV']\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    environment=PINECONE_API_ENV\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Store the vectors to the specified Pinecone index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the index name which can be stored in pinecone.io as long-term memory \n",
    "index_name = \"langchaints\"  # example pinecone index; replace by yours\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(name=index_name, dimension=1536, metric=\"cosine\", shards=1)\n",
    "docsearch = Pinecone.from_texts([t.page_content for t in all_texts], embeddings, index_name=index_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use Open AI LLM with gpt-3.5-turbo. Set the temperature to be 0 if you do not want it to make up things. And set up to use the ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", openai_api_key=OPENAI_API_KEY)\n",
    "CRqa = ConversationalRetrievalChain.from_llm(llm, retriever=docsearch.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initialize chat history. Provide a prompt and generate a reply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "query = \"What is the summary of the documents on RS?\"\n",
    "result = CRqa({\"question\": query, \"chat_history\": chat_history})\n",
    "result['answer']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Update the chat history, and provide another prompt. Generate a reply and also print out the source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [(query, result[\"answer\"])]\n",
    "query = \"How is it defined in the context?\"\n",
    "result = CRqa({\"question\": query, \"chat_history\": chat_history})\n",
    "result['answer']\n",
    "if result['source_documents'][0].metadata:\n",
    "    print(result['source_documents'][0].metadata['source'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
